# Cloud Build configuration for LLM-Inference-Gateway
# Builds and deploys unified service to Cloud Run

substitutions:
  _SERVICE_NAME: llm-inference-gateway
  _REGION: us-central1
  _PLATFORM_ENV: dev
  _MIN_INSTANCES: '0'
  _MAX_INSTANCES: '10'
  _MEMORY: '1Gi'
  _CPU: '1'
  _CONCURRENCY: '80'
  _TIMEOUT: '300s'

steps:
  # Step 1: Build container image (tests run in Dockerfile)
  - name: 'gcr.io/cloud-builders/docker'
    id: 'build'
    args:
      - 'build'
      - '-t'
      - 'gcr.io/$PROJECT_ID/${_SERVICE_NAME}:$BUILD_ID'
      - '-t'
      - 'gcr.io/$PROJECT_ID/${_SERVICE_NAME}:latest'
      - '-t'
      - 'gcr.io/$PROJECT_ID/${_SERVICE_NAME}:${_PLATFORM_ENV}'
      - '-f'
      - 'deploy/cloud-run/Dockerfile'
      - '.'

  # Step 3: Push to Container Registry
  - name: 'gcr.io/cloud-builders/docker'
    id: 'push'
    args:
      - 'push'
      - '--all-tags'
      - 'gcr.io/$PROJECT_ID/${_SERVICE_NAME}'
    waitFor: ['build']

  # Step 3: Deploy to Cloud Run
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: 'deploy'
    entrypoint: 'gcloud'
    args:
      - 'run'
      - 'deploy'
      - '${_SERVICE_NAME}'
      - '--image'
      - 'gcr.io/$PROJECT_ID/${_SERVICE_NAME}:latest'
      - '--region'
      - '${_REGION}'
      - '--platform'
      - 'managed'
      - '--memory'
      - '${_MEMORY}'
      - '--cpu'
      - '${_CPU}'
      - '--concurrency'
      - '${_CONCURRENCY}'
      - '--timeout'
      - '${_TIMEOUT}'
      - '--min-instances'
      - '${_MIN_INSTANCES}'
      - '--max-instances'
      - '${_MAX_INSTANCES}'
      - '--port'
      - '8080'
      - '--allow-unauthenticated'
      - '--set-env-vars'
      - 'PLATFORM_ENV=${_PLATFORM_ENV},SERVICE_NAME=${_SERVICE_NAME},SERVICE_VERSION=$BUILD_ID'
      - '--set-secrets'
      - 'RUVECTOR_API_KEY=ruvector-api-key:latest,OPENAI_API_KEY=openai-api-key:latest,ANTHROPIC_API_KEY=anthropic-api-key:latest'
      - '--service-account'
      - 'inference-gateway@$PROJECT_ID.iam.gserviceaccount.com'
      - '--labels'
      - 'service=llm-inference-gateway,env=${_PLATFORM_ENV},team=agentics'
    waitFor: ['push']

  # Step 5: Verify deployment
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: 'verify'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        SVC_URL=$$(gcloud run services describe ${_SERVICE_NAME} --region=${_REGION} --format='value(status.url)')
        echo "Service URL: $$SVC_URL"

        # Health check
        curl -f "$$SVC_URL/health" || exit 1
        echo "Health check passed"

        # Agent endpoint check
        curl -f "$$SVC_URL/agents" || exit 1
        echo "Agent endpoint accessible"

        echo "Deployment verification complete"
    waitFor: ['deploy']

images:
  - 'gcr.io/$PROJECT_ID/${_SERVICE_NAME}:latest'
  - 'gcr.io/$PROJECT_ID/${_SERVICE_NAME}:${_PLATFORM_ENV}'

options:
  logging: CLOUD_LOGGING_ONLY
  machineType: 'E2_HIGHCPU_8'

timeout: '1800s'

tags:
  - 'llm-inference-gateway'
  - '${_PLATFORM_ENV}'
