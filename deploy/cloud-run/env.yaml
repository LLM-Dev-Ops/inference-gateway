# LLM-Inference-Gateway Environment Configuration
# Environment-specific values for Cloud Run deployment

# Development Environment
dev:
  # Service Identity
  SERVICE_NAME: llm-inference-gateway
  SERVICE_VERSION: dev
  PLATFORM_ENV: dev

  # Server
  GATEWAY_HOST: "0.0.0.0"
  GATEWAY_PORT: "8080"
  RUST_LOG: "debug,tower_http=debug,gateway=trace"

  # RuVector Service (Persistence)
  RUVECTOR_SERVICE_URL: "https://ruvector-service-agentics-dev.run.app"
  # RUVECTOR_API_KEY: from Secret Manager

  # Telemetry (LLM-Observatory)
  TELEMETRY_ENDPOINT: "https://llm-observatory-agentics-dev.run.app/v1/telemetry"
  OTEL_ENABLED: "true"
  OTEL_SERVICE_NAME: llm-inference-gateway-dev
  OTEL_EXPORTER_OTLP_ENDPOINT: "https://otel-collector-agentics-dev.run.app"

  # Resilience (relaxed for dev)
  CIRCUIT_BREAKER_FAILURE_THRESHOLD: "10"
  CIRCUIT_BREAKER_TIMEOUT_SECONDS: "60"
  RATE_LIMIT_ENABLED: "false"
  RATE_LIMIT_REQUESTS_PER_MINUTE: "10000"

  # Scaling
  MIN_INSTANCES: "0"
  MAX_INSTANCES: "3"
  MEMORY: "512Mi"
  CPU: "1"

# Staging Environment
staging:
  SERVICE_NAME: llm-inference-gateway
  SERVICE_VERSION: staging
  PLATFORM_ENV: staging

  GATEWAY_HOST: "0.0.0.0"
  GATEWAY_PORT: "8080"
  RUST_LOG: "info,tower_http=info"

  RUVECTOR_SERVICE_URL: "https://ruvector-service-agentics-staging.run.app"

  TELEMETRY_ENDPOINT: "https://llm-observatory-agentics-staging.run.app/v1/telemetry"
  OTEL_ENABLED: "true"
  OTEL_SERVICE_NAME: llm-inference-gateway-staging
  OTEL_EXPORTER_OTLP_ENDPOINT: "https://otel-collector-agentics-staging.run.app"

  CIRCUIT_BREAKER_FAILURE_THRESHOLD: "5"
  CIRCUIT_BREAKER_TIMEOUT_SECONDS: "30"
  RATE_LIMIT_ENABLED: "true"
  RATE_LIMIT_REQUESTS_PER_MINUTE: "5000"

  MIN_INSTANCES: "1"
  MAX_INSTANCES: "10"
  MEMORY: "1Gi"
  CPU: "1"

# Production Environment
prod:
  SERVICE_NAME: llm-inference-gateway
  SERVICE_VERSION: prod
  PLATFORM_ENV: prod

  GATEWAY_HOST: "0.0.0.0"
  GATEWAY_PORT: "8080"
  RUST_LOG: "info,tower_http=warn"

  RUVECTOR_SERVICE_URL: "https://ruvector-service-agentics-prod.run.app"

  TELEMETRY_ENDPOINT: "https://llm-observatory-agentics-prod.run.app/v1/telemetry"
  OTEL_ENABLED: "true"
  OTEL_SERVICE_NAME: llm-inference-gateway-prod
  OTEL_EXPORTER_OTLP_ENDPOINT: "https://otel-collector-agentics-prod.run.app"

  CIRCUIT_BREAKER_FAILURE_THRESHOLD: "5"
  CIRCUIT_BREAKER_TIMEOUT_SECONDS: "30"
  RATE_LIMIT_ENABLED: "true"
  RATE_LIMIT_REQUESTS_PER_MINUTE: "1000"

  MIN_INSTANCES: "2"
  MAX_INSTANCES: "100"
  MEMORY: "2Gi"
  CPU: "2"

# Secrets (always from Secret Manager, never inline)
secrets:
  required:
    - ruvector-api-key       # RuVector service authentication
    - openai-api-key         # OpenAI provider
    - anthropic-api-key      # Anthropic provider
    - google-api-key         # Google/Gemini provider
  optional:
    - azure-openai-key       # Azure OpenAI
    - aws-access-key-id      # AWS Bedrock
    - aws-secret-access-key  # AWS Bedrock
