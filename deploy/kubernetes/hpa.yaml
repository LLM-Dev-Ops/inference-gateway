# LLM Inference Gateway Horizontal Pod Autoscaler
# Automatically scales pods based on CPU, memory, and custom metrics

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: llm-gateway-hpa
  namespace: llm-gateway
  labels:
    app.kubernetes.io/name: llm-inference-gateway
    app.kubernetes.io/component: autoscaling
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: llm-gateway

  # Scaling bounds
  minReplicas: 3
  maxReplicas: 20

  # Scaling metrics
  metrics:
    # CPU-based scaling
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70

    # Memory-based scaling
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80

    # Custom metrics (requires metrics-server and custom metrics adapter)
    # Uncomment and configure based on your metrics stack
    #
    # Request rate based scaling (Prometheus adapter)
    # - type: Pods
    #   pods:
    #     metric:
    #       name: http_requests_per_second
    #     target:
    #       type: AverageValue
    #       averageValue: "1000"
    #
    # Queue depth based scaling
    # - type: External
    #   external:
    #     metric:
    #       name: llm_gateway_queue_depth
    #       selector:
    #         matchLabels:
    #           service: llm-gateway
    #     target:
    #       type: AverageValue
    #       averageValue: "50"

  # Scaling behavior for fine-grained control
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        # Scale up by 100% of current replicas
        - type: Percent
          value: 100
          periodSeconds: 60
        # Or scale up by 4 pods
        - type: Pods
          value: 4
          periodSeconds: 60
      selectPolicy: Max

    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        # Scale down by 50% of current replicas
        - type: Percent
          value: 50
          periodSeconds: 120
        # Or scale down by 2 pods
        - type: Pods
          value: 2
          periodSeconds: 120
      selectPolicy: Min

---
# Vertical Pod Autoscaler (optional - for right-sizing resources)
# Requires VPA to be installed in the cluster
# apiVersion: autoscaling.k8s.io/v1
# kind: VerticalPodAutoscaler
# metadata:
#   name: llm-gateway-vpa
#   namespace: llm-gateway
#   labels:
#     app.kubernetes.io/name: llm-inference-gateway
#     app.kubernetes.io/component: autoscaling
# spec:
#   targetRef:
#     apiVersion: apps/v1
#     kind: Deployment
#     name: llm-gateway
#   updatePolicy:
#     updateMode: "Auto"  # Options: Off, Initial, Recreate, Auto
#   resourcePolicy:
#     containerPolicies:
#       - containerName: gateway
#         minAllowed:
#           cpu: "250m"
#           memory: "256Mi"
#         maxAllowed:
#           cpu: "4000m"
#           memory: "8Gi"
#         controlledResources: ["cpu", "memory"]
#         controlledValues: RequestsAndLimits

