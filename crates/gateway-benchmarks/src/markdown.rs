//! Markdown report generation for benchmark results.

use crate::BenchmarkResult;
use chrono::Utc;

/// Generate a summary markdown report from benchmark results.
///
/// # Arguments
///
/// * `results` - Slice of benchmark results to summarize
///
/// # Returns
///
/// A formatted markdown string containing the benchmark summary.
pub fn generate_summary(results: &[BenchmarkResult]) -> String {
    let mut md = String::new();

    // Header
    md.push_str("# Benchmark Summary\n\n");
    md.push_str(&format!(
        "**Generated:** {}\n\n",
        Utc::now().format("%Y-%m-%d %H:%M:%S UTC")
    ));

    // Overview statistics
    let total = results.len();
    let passed = results.iter().filter(|r| !r.is_error()).count();
    let failed = total - passed;

    md.push_str("## Overview\n\n");
    md.push_str(&format!("| Metric | Value |\n"));
    md.push_str(&format!("|--------|-------|\n"));
    md.push_str(&format!("| Total Benchmarks | {} |\n", total));
    md.push_str(&format!("| Passed | {} |\n", passed));
    md.push_str(&format!("| Failed | {} |\n", failed));
    md.push_str(&format!(
        "| Success Rate | {:.1}% |\n\n",
        if total > 0 {
            (passed as f64 / total as f64) * 100.0
        } else {
            0.0
        }
    ));

    // Results table
    md.push_str("## Results\n\n");
    md.push_str("| Target | Status | Latency (ms) | Throughput (rps) | Notes |\n");
    md.push_str("|--------|--------|--------------|------------------|-------|\n");

    for result in results {
        let status = if result.is_error() { "❌ Failed" } else { "✅ Passed" };
        let latency = result
            .latency_ms()
            .map(|v| format!("{:.2}", v))
            .unwrap_or_else(|| "-".to_string());
        let throughput = result
            .throughput_rps()
            .map(|v| format!("{:.0}", v))
            .unwrap_or_else(|| "-".to_string());
        let notes = if result.is_error() {
            result
                .metrics
                .get("error")
                .and_then(|e| e.as_str())
                .unwrap_or("Unknown error")
                .to_string()
        } else {
            extract_notes(&result.metrics)
        };

        md.push_str(&format!(
            "| {} | {} | {} | {} | {} |\n",
            result.target_id, status, latency, throughput, notes
        ));
    }

    md.push_str("\n");

    // Detailed results
    md.push_str("## Detailed Results\n\n");

    for result in results {
        md.push_str(&format!("### {}\n\n", result.target_id));
        md.push_str(&format!(
            "**Timestamp:** {}\n\n",
            result.timestamp.format("%Y-%m-%d %H:%M:%S UTC")
        ));
        md.push_str("**Metrics:**\n\n");
        md.push_str("```json\n");
        md.push_str(&serde_json::to_string_pretty(&result.metrics).unwrap_or_default());
        md.push_str("\n```\n\n");
    }

    // Footer
    md.push_str("---\n\n");
    md.push_str("*Generated by gateway-benchmarks canonical benchmark system*\n");

    md
}

/// Extract notable information from metrics for the summary table.
fn extract_notes(metrics: &serde_json::Value) -> String {
    let mut notes = Vec::new();

    if let Some(iterations) = metrics.get("iterations").and_then(|v| v.as_u64()) {
        notes.push(format!("{} iterations", iterations));
    }

    if let Some(p99) = metrics.get("p99_ms").and_then(|v| v.as_f64()) {
        notes.push(format!("p99: {:.2}ms", p99));
    }

    if notes.is_empty() {
        "-".to_string()
    } else {
        notes.join(", ")
    }
}

/// Generate a detailed markdown report for a single benchmark result.
pub fn generate_detailed_report(result: &BenchmarkResult) -> String {
    let mut md = String::new();

    md.push_str(&format!("# Benchmark Report: {}\n\n", result.target_id));
    md.push_str(&format!(
        "**Executed:** {}\n\n",
        result.timestamp.format("%Y-%m-%d %H:%M:%S UTC")
    ));

    md.push_str("## Status\n\n");
    if result.is_error() {
        md.push_str("❌ **FAILED**\n\n");
        if let Some(error) = result.metrics.get("error").and_then(|e| e.as_str()) {
            md.push_str(&format!("**Error:** {}\n\n", error));
        }
    } else {
        md.push_str("✅ **PASSED**\n\n");
    }

    md.push_str("## Metrics\n\n");

    // Format known metrics nicely
    if let Some(latency) = result.latency_ms() {
        md.push_str(&format!("- **Latency:** {:.2} ms\n", latency));
    }
    if let Some(throughput) = result.throughput_rps() {
        md.push_str(&format!("- **Throughput:** {:.0} requests/sec\n", throughput));
    }
    if let Some(iterations) = result.metrics.get("iterations").and_then(|v| v.as_u64()) {
        md.push_str(&format!("- **Iterations:** {}\n", iterations));
    }

    // Percentiles
    for percentile in &["p50_ms", "p90_ms", "p95_ms", "p99_ms"] {
        if let Some(value) = result.metrics.get(*percentile).and_then(|v| v.as_f64()) {
            md.push_str(&format!("- **{}:** {:.2} ms\n", percentile, value));
        }
    }

    md.push_str("\n## Raw Metrics\n\n");
    md.push_str("```json\n");
    md.push_str(&serde_json::to_string_pretty(&result.metrics).unwrap_or_default());
    md.push_str("\n```\n");

    md
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_generate_summary_empty() {
        let results = vec![];
        let summary = generate_summary(&results);
        assert!(summary.contains("# Benchmark Summary"));
        assert!(summary.contains("Total Benchmarks | 0"));
    }

    #[test]
    fn test_generate_summary_with_results() {
        let results = vec![
            BenchmarkResult::new(
                "test_1",
                serde_json::json!({
                    "latency_ms": 10.5,
                    "throughput_rps": 1000
                }),
            ),
            BenchmarkResult::new(
                "test_2",
                serde_json::json!({
                    "error": "Failed",
                    "status": "failed"
                }),
            ),
        ];

        let summary = generate_summary(&results);
        assert!(summary.contains("Total Benchmarks | 2"));
        assert!(summary.contains("Passed | 1"));
        assert!(summary.contains("Failed | 1"));
        assert!(summary.contains("test_1"));
        assert!(summary.contains("test_2"));
    }

    #[test]
    fn test_generate_detailed_report() {
        let result = BenchmarkResult::new(
            "detailed_test",
            serde_json::json!({
                "latency_ms": 15.5,
                "throughput_rps": 500,
                "iterations": 1000,
                "p99_ms": 25.0
            }),
        );

        let report = generate_detailed_report(&result);
        assert!(report.contains("# Benchmark Report: detailed_test"));
        assert!(report.contains("✅ **PASSED**"));
        assert!(report.contains("Latency:** 15.50 ms"));
    }
}
